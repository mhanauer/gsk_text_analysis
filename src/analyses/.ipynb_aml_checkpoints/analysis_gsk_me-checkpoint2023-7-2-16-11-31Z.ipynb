{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%load_ext lab_black"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step: Load data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import nltk\n",
        "from pyprojroot import here\n",
        "\n",
        "from skimpy import clean_columns\n",
        "from data_cleaning.fun_hot_encode_limit import fun_hot_encode_limit\n",
        "\n",
        "path_data = here(\"./data\")\n",
        "os.chdir(path_data)\n",
        "data_ra = pd.read_csv(\"ra_data.csv\")\n",
        "data_ra = clean_columns(data_ra)\n",
        "\n",
        "nltk.download(\"vader_lexicon\")\n",
        "data_ra[\"date\"] = pd.to_datetime(data_ra[\"date\"])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1;36m11\u001b[0m column names have been cleaned\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> column names have been cleaned\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n/tmp/ipykernel_27975/814015519.py:16: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  data_ra[\"date\"] = pd.to_datetime(data_ra[\"date\"])\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690988315812
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Let's assume your DataFrame is called df and the column with the text data is 'reviews'\n",
        "data_ra[\"sentiment_scores\"] = data_ra[\"reviews\"].apply(\n",
        "    lambda review: sia.polarity_scores(review)\n",
        ")\n",
        "\n",
        "# The above will return a dictionary with 'compound', 'neg', 'neu', 'pos' as keys.\n",
        "# If you're interested in overall sentiment, you could use 'compound' score which is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive).\n",
        "# You can create a new column for this score.\n",
        "\n",
        "data_ra[\"compound_score\"] = data_ra[\"sentiment_scores\"].apply(\n",
        "    lambda score_dict: score_dict[\"compound\"]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690988318975
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_ra_sentiment = data_ra.copy()"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690988321035
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_ra_sentiment[\"compound_score\"].min())\n",
        "data_ra_sentiment[\"compound_score\"].describe()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "-0.9803\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "count    185.000000\nmean      -0.013512\nstd        0.609303\nmin       -0.980300\n25%       -0.539900\n50%        0.000000\n75%        0.542300\nmax        0.991500\nName: compound_score, dtype: float64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690990274880
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Let's say your DataFrame is called df and the column with sentiment scores is 'compound_score'\n",
        "t_statistic, p_value = stats.ttest_1samp(data_ra_sentiment[\"compound_score\"], 0)\n",
        "\n",
        "print(\"t statistic:\", t_statistic.round(2))\n",
        "print(\"p value:\", p_value.round(2))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "t statistic: -0.3\np value: 0.76\n"
        }
      ],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690990036405
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Tokenize the text\n",
        "    word_tokens = word_tokenize(text)\n",
        "    # Remove stopwords and lemmatize\n",
        "    filtered_text = [\n",
        "        lemmatizer.lemmatize(word) for word in word_tokens if word not in stop_words\n",
        "    ]\n",
        "    return \" \".join(filtered_text)\n",
        "\n",
        "\n",
        "# Preprocess the reviews\n",
        "data_ra_sentiment[\"reviews_clean\"] = (\n",
        "    data_ra_sentiment[\"reviews\"].apply(preprocess_text).values\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
        }
      ],
      "execution_count": 55,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690991779024
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_ra_sentiment"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the zero-shot classification pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "\n",
        "# Define the candidate labels\n",
        "candidate_labels = [\"pain management\"]\n",
        "\n",
        "\n",
        "# Function to apply the classifier to a review\n",
        "def classify_review(review):\n",
        "    result = classifier(review, candidate_labels)\n",
        "    return dict(zip(result[\"labels\"], result[\"scores\"]))\n",
        "\n",
        "\n",
        "# Apply the classifier to each review\n",
        "data_ra_sentiment[\"emotion_scores\"] = data_ra_sentiment[\"reviews\"].apply(\n",
        "    classify_review\n",
        ")\n",
        "\n",
        "# Create a new column for each label\n",
        "for label in candidate_labels:\n",
        "    data_ra_sentiment[label] = data_ra_sentiment[\"emotion_scores\"].apply(\n",
        "        lambda scores: scores.get(label)\n",
        "    )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "No model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/1.13k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83a0e1cf539b4d6984b1e7f350eb3d46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/1.52G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40121173f1d34829b763b0a17c531fe6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa08a1746ded49d994d113b193df5246"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77890596eb344e03a5e5059cca3d3f43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbc30e8cf7e145f58013f3c027209226"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2595830914e47c0a461de01d19dc999"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690991161336
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_ra_sentiment = clean_columns(data_ra_sentiment)\n",
        "len_pain = (\n",
        "    data_ra_sentiment.query(\"pain_management >= .75\").reset_index(drop=True).shape[0]\n",
        ")\n",
        "len_all = data_ra_sentiment.shape[0]\n",
        "percent_pain = round(len_pain / len_all, 2)\n",
        "data_pain = pd.DataFrame(\n",
        "    {\"len_pain\": len_pain, \"len_all\": len_all, \"percent_pain\": percent_pain}, index=[0]\n",
        ")\n",
        "\n",
        "data_pain"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1;36m0\u001b[0m column names have been cleaned\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> column names have been cleaned\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 50,
          "data": {
            "text/plain": "   len_pain  len_all  percent_pain\n0        92      185           0.5",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>len_pain</th>\n      <th>len_all</th>\n      <th>percent_pain</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>92</td>\n      <td>185</td>\n      <td>0.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690991443392
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_ra_pain = data_ra_sentiment.query(\"pain_management >= .75\").reset_index(drop=True)"
      ],
      "outputs": [],
      "execution_count": 57,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690992116997
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Download the necessary NLTK data\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Add custom words to stop words\n",
        "custom_stop_words = [\"humira\", \"syringe\", \"kit\", \"pen\", \"injector\"]\n",
        "stop_words.update(custom_stop_words)\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Tokenize the text\n",
        "    word_tokens = word_tokenize(text)\n",
        "    # Remove stopwords and lemmatize\n",
        "    filtered_text = [\n",
        "        lemmatizer.lemmatize(word) for word in word_tokens if word not in stop_words\n",
        "    ]\n",
        "    return \" \".join(filtered_text)\n",
        "\n",
        "\n",
        "# Preprocess the reviews\n",
        "reviews = data_ra_pain[\"reviews_clean\"].apply(preprocess_text).values\n",
        "\n",
        "# Create BERTopic model\n",
        "topic_model = BERTopic(language=\"english\")\n",
        "\n",
        "# Fit the model to the reviews\n",
        "topics, _ = topic_model.fit_transform(reviews)\n",
        "\n",
        "# Get the topic frequencies\n",
        "topic_info = topic_model.get_topic_info()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
        }
      ],
      "execution_count": 65,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690992436442
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_info.to_csv(\"\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 67,
          "data": {
            "text/plain": "0    [since january 2009 taking without mtx taking ...\n1    [diagnosed 4 year ago started methotrexate emb...\n2    ['ve using 3 year . made remarkable improvemen...\nName: Representative_Docs, dtype: object"
          },
          "metadata": {}
        }
      ],
      "execution_count": 67,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690992581335
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}